{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for generating names, starting with the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "class CharacterEncoder(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transform a string into context and target character sequence numbers, using the ordinal\n",
    "    value of each character.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.context_length = context_length\n",
    "        self.maximum_ordinal = maximum_ordinal\n",
    "        \n",
    "    def fit(self, strings, **kwargs):\n",
    "        '''\n",
    "        No need to fit.\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform an iterable source of strings into a dense matrix\n",
    "        of character identifiers.\n",
    "        \n",
    "        Each sample will be a string snippet of context_length characters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character identifier, and\n",
    "            a one dimensional [sample_index] with a 32 bit character identifier to predict.\n",
    "        '''\n",
    "        # forgive passing a single string\n",
    "        if type(strings) is str:\n",
    "            strings = [strings]\n",
    "        # buffer up contexts and targets, we'll be predicting target characters\n",
    "        # from context strings\n",
    "        contexts = []\n",
    "        targets = []\n",
    "        for i, string in enumerate(strings):\n",
    "            # lowercase and stripped of leading whitespace, makes the model more compact\n",
    "            string = string.lower().strip()\n",
    "            # if the string is too short -- pad it with leading spaces\n",
    "            if len(string) <= self.context_length:\n",
    "                string = ' ' * (1 + self.context_length - len(string)) + string\n",
    "            # null character termination for each string\n",
    "            string += chr(0)\n",
    "            for j in range(0, len(string) - self.context_length):\n",
    "                contexts.append(string[j:j + self.context_length])\n",
    "                targets.append(string[j + self.context_length])\n",
    "        # blocks of memory to hold character ordinals\n",
    "        X = np.zeros((len(contexts), self.context_length), dtype=np.int32)\n",
    "        Y = np.zeros(len(targets), dtype=np.int32)\n",
    "        # numerical encoding of character values\n",
    "        for i, context in enumerate(contexts):\n",
    "            for j, character in enumerate(context):\n",
    "                X[i, j] = min(ord(character), self.maximum_ordinal)\n",
    "        for i, character in enumerate(targets):\n",
    "            Y[i] = min(ord(character), self.maximum_ordinal)\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLanguageModelVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Base language model uses a CharacterEncoder to create character ordinals\n",
    "    and then applies a transformation in order to create vectors.\n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.sequencer = CharacterEncoder(context_length, maximum_ordinal)\n",
    "    \n",
    "    def fit(self, strings):\n",
    "        '''\n",
    "        Nothing to fit.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a dense (X, Y) pairing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) three dimensional [sample_index, character_index, one_hot] context X and\n",
    "            a two dimensional [sample_index, one_hot] target Y.\n",
    "        '''\n",
    "        # character sequence numbers\n",
    "        X, Y = self.sequencer.transform(strings)\n",
    "        # one hot context encoding\n",
    "        x = np.zeros((X.shape[0], self.sequencer.context_length, self.sequencer.maximum_ordinal), dtype=np.bool)\n",
    "        y = np.zeros((Y.shape[0], self.sequencer.maximum_ordinal), dtype=np.bool)\n",
    "        for i, context in enumerate(X):\n",
    "            for t, char in enumerate(context):\n",
    "                x[i, t, char] = 1\n",
    "        for i, target in enumerate(Y):\n",
    "            y[i, target] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of one hot encodings, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        ordinals = X.argmax(-1)\n",
    "        decoder = np.vectorize(chr)\n",
    "        # allow for single characters or enumerable sets of characters\n",
    "        decoded = np.array([decoder(ordinals)])\n",
    "        return ' '.join(decoded.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CharacterLanguageModelVectorizer()\n",
    "with open('cities.txt', encoding='utf8') as cities:\n",
    "    X, Y = vectorizer.fit_transform(cities.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ..., \n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]], dtype=bool),\n",
       " array([False, False, False, ..., False, False, False], dtype=bool),\n",
       " 101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0], Y[0].argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build up a recurrent neural network to learn a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wballard/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, CuDNNLSTM, Dropout, Dense, Reshape, BatchNormalization\n",
    "\n",
    "class RecurrentLanguageModel(BaseEstimator):\n",
    "    '''\n",
    "    Create a language model with a neural network and normalized character encoding.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vectorizer, hidden_layers=256, gpu_optimized=False):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        vectorizer : transformer\n",
    "            Object to transform input strings into numerical encodings.\n",
    "        hidden_layers : int\n",
    "            Size of the model's hidden layer, controls complexity.\n",
    "        gpu_optimized : bool\n",
    "            If True, use special code in keras to boost performance.\n",
    "        '''\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.gpu_optimized = gpu_optimized\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def fit(self, strings, epochs=256, batch_size=256):\n",
    "        '''\n",
    "        Create and fit a model to the passed in strings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable source of string text.\n",
    "        '''\n",
    "        if self.gpu_optimized:\n",
    "            RNN = CuDNNLSTM\n",
    "        else:\n",
    "            RNN = LSTM\n",
    "        X, Y = self.vectorizer.transform(strings)\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.model = model = Sequential()\n",
    "        # input shape is represented as the shape of a single batch entry\n",
    "        model.add(RNN(self.hidden_layers, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(RNN(self.hidden_layers))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.hidden_layers, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.hidden_layers, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this model with cities. I'm using a GPU - and very much recommend you do so! You can set the optimization to False if you need to use a CPU.\n",
    "\n",
    "City names aren't sentences, so we need to use a relatively short context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "107270/107270 [==============================] - 299s 3ms/step - loss: 2.1651\n",
      "Epoch 2/256\n",
      "107270/107270 [==============================] - 8s 76us/step - loss: 1.6249\n",
      "Epoch 3/256\n",
      "107270/107270 [==============================] - 8s 77us/step - loss: 1.5012\n",
      "Epoch 4/256\n",
      "107270/107270 [==============================] - 8s 76us/step - loss: 1.4298\n",
      "Epoch 5/256\n",
      "107270/107270 [==============================] - 8s 76us/step - loss: 1.3790\n",
      "Epoch 6/256\n",
      "107270/107270 [==============================] - 8s 72us/step - loss: 1.3141\n",
      "Epoch 7/256\n",
      "107270/107270 [==============================] - 8s 72us/step - loss: 1.2799\n",
      "Epoch 8/256\n",
      "107270/107270 [==============================] - 8s 70us/step - loss: 1.2286\n",
      "Epoch 9/256\n",
      "107270/107270 [==============================] - 8s 70us/step - loss: 1.1879\n",
      "Epoch 10/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 1.1609\n",
      "Epoch 11/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 1.0927\n",
      "Epoch 12/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 1.0895\n",
      "Epoch 13/256\n",
      "107270/107270 [==============================] - 7s 68us/step - loss: 1.0628\n",
      "Epoch 14/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 1.0124\n",
      "Epoch 15/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 1.0540\n",
      "Epoch 16/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.9580\n",
      "Epoch 17/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.9727\n",
      "Epoch 18/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.9342\n",
      "Epoch 19/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.8854\n",
      "Epoch 20/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.8624\n",
      "Epoch 21/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.9717\n",
      "Epoch 22/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.8720\n",
      "Epoch 23/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.9037\n",
      "Epoch 24/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.8799\n",
      "Epoch 25/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7995\n",
      "Epoch 26/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.8161\n",
      "Epoch 27/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7905\n",
      "Epoch 28/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.8248\n",
      "Epoch 29/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7727\n",
      "Epoch 30/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.7916\n",
      "Epoch 31/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7889\n",
      "Epoch 32/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7672\n",
      "Epoch 33/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7532\n",
      "Epoch 34/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7704\n",
      "Epoch 35/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.7761\n",
      "Epoch 36/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.7500\n",
      "Epoch 37/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.7560\n",
      "Epoch 38/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7452\n",
      "Epoch 39/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.7245\n",
      "Epoch 40/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.7282\n",
      "Epoch 41/256\n",
      "107270/107270 [==============================] - 6s 61us/step - loss: 0.6949\n",
      "Epoch 42/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.7117\n",
      "Epoch 43/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.7206\n",
      "Epoch 44/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7247\n",
      "Epoch 45/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.7219\n",
      "Epoch 46/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.7386\n",
      "Epoch 47/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.7323\n",
      "Epoch 48/256\n",
      "107270/107270 [==============================] - 8s 70us/step - loss: 0.6627\n",
      "Epoch 49/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6782\n",
      "Epoch 50/256\n",
      "107270/107270 [==============================] - 7s 68us/step - loss: 0.6855\n",
      "Epoch 51/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6667\n",
      "Epoch 52/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7012\n",
      "Epoch 53/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.6666\n",
      "Epoch 54/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.7112\n",
      "Epoch 55/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6959\n",
      "Epoch 56/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.6576\n",
      "Epoch 57/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.6740\n",
      "Epoch 58/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6884\n",
      "Epoch 59/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.6666\n",
      "Epoch 60/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6453\n",
      "Epoch 61/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6482\n",
      "Epoch 62/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6457\n",
      "Epoch 63/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.6763\n",
      "Epoch 64/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6336\n",
      "Epoch 65/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6895\n",
      "Epoch 66/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.6371\n",
      "Epoch 67/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6585\n",
      "Epoch 68/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6467\n",
      "Epoch 69/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6349\n",
      "Epoch 70/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6383\n",
      "Epoch 71/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6726\n",
      "Epoch 72/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6318\n",
      "Epoch 73/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6724\n",
      "Epoch 74/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6653\n",
      "Epoch 75/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.6545\n",
      "Epoch 76/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6086\n",
      "Epoch 77/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6444\n",
      "Epoch 78/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6202\n",
      "Epoch 79/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6214\n",
      "Epoch 80/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6292\n",
      "Epoch 81/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6164\n",
      "Epoch 82/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6378\n",
      "Epoch 83/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6121\n",
      "Epoch 84/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6194\n",
      "Epoch 85/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6132\n",
      "Epoch 86/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.6187\n",
      "Epoch 87/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6165\n",
      "Epoch 88/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.6903\n",
      "Epoch 89/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6238\n",
      "Epoch 90/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6204\n",
      "Epoch 91/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6536\n",
      "Epoch 92/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6019\n",
      "Epoch 93/256\n",
      "107270/107270 [==============================] - 7s 66us/step - loss: 0.6001\n",
      "Epoch 94/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6227\n",
      "Epoch 95/256\n",
      "107270/107270 [==============================] - 6s 58us/step - loss: 0.6067\n",
      "Epoch 96/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5993\n",
      "Epoch 97/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6566\n",
      "Epoch 98/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6366\n",
      "Epoch 99/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6449\n",
      "Epoch 100/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6157\n",
      "Epoch 101/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6424\n",
      "Epoch 102/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5942\n",
      "Epoch 103/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5884\n",
      "Epoch 104/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6142\n",
      "Epoch 105/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6071\n",
      "Epoch 106/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5817\n",
      "Epoch 107/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6079\n",
      "Epoch 108/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6331\n",
      "Epoch 109/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5912\n",
      "Epoch 110/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6006\n",
      "Epoch 111/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5802\n",
      "Epoch 112/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6069\n",
      "Epoch 113/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5984\n",
      "Epoch 114/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5901\n",
      "Epoch 115/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6287\n",
      "Epoch 116/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6154\n",
      "Epoch 117/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5796\n",
      "Epoch 118/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5994\n",
      "Epoch 119/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5883\n",
      "Epoch 120/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5851\n",
      "Epoch 121/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5863\n",
      "Epoch 122/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.6183\n",
      "Epoch 123/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6265\n",
      "Epoch 124/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5754\n",
      "Epoch 125/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5810\n",
      "Epoch 126/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6481\n",
      "Epoch 127/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.6037\n",
      "Epoch 128/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6004\n",
      "Epoch 129/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5868\n",
      "Epoch 130/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5868\n",
      "Epoch 131/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5881\n",
      "Epoch 132/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5814\n",
      "Epoch 133/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6013\n",
      "Epoch 134/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5809\n",
      "Epoch 135/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5989\n",
      "Epoch 136/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6269\n",
      "Epoch 137/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5794\n",
      "Epoch 138/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5800\n",
      "Epoch 139/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5737\n",
      "Epoch 140/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5870\n",
      "Epoch 141/256\n",
      "107270/107270 [==============================] - 6s 61us/step - loss: 0.5608\n",
      "Epoch 142/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6282\n",
      "Epoch 143/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6042\n",
      "Epoch 144/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5756\n",
      "Epoch 145/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5755\n",
      "Epoch 146/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5814\n",
      "Epoch 147/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5849\n",
      "Epoch 148/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5663\n",
      "Epoch 149/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5692\n",
      "Epoch 150/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6194\n",
      "Epoch 151/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5848\n",
      "Epoch 152/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6052\n",
      "Epoch 153/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5632\n",
      "Epoch 154/256\n",
      "107270/107270 [==============================] - 7s 66us/step - loss: 0.5521\n",
      "Epoch 155/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5727\n",
      "Epoch 156/256\n",
      "107270/107270 [==============================] - 7s 66us/step - loss: 0.5814\n",
      "Epoch 157/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5750\n",
      "Epoch 158/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5906\n",
      "Epoch 159/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5807\n",
      "Epoch 160/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5724\n",
      "Epoch 161/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5675\n",
      "Epoch 162/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5822\n",
      "Epoch 163/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5846\n",
      "Epoch 164/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5559\n",
      "Epoch 165/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5994\n",
      "Epoch 166/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6460\n",
      "Epoch 167/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5845\n",
      "Epoch 168/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5621\n",
      "Epoch 169/256\n",
      "107270/107270 [==============================] - 6s 57us/step - loss: 0.5631\n",
      "Epoch 170/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5830\n",
      "Epoch 171/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5658\n",
      "Epoch 172/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5628\n",
      "Epoch 173/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5701\n",
      "Epoch 174/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6277\n",
      "Epoch 175/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6037\n",
      "Epoch 176/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5568\n",
      "Epoch 177/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5793\n",
      "Epoch 178/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5709\n",
      "Epoch 179/256\n",
      "107270/107270 [==============================] - 7s 68us/step - loss: 0.5569\n",
      "Epoch 180/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5624\n",
      "Epoch 181/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5700\n",
      "Epoch 182/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5566\n",
      "Epoch 183/256\n",
      "107270/107270 [==============================] - 6s 61us/step - loss: 0.5766\n",
      "Epoch 184/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5715\n",
      "Epoch 185/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5647\n",
      "Epoch 186/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6233\n",
      "Epoch 187/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5725\n",
      "Epoch 188/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6140\n",
      "Epoch 189/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.6101\n",
      "Epoch 190/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5668\n",
      "Epoch 191/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5587\n",
      "Epoch 192/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.6145\n",
      "Epoch 193/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5860\n",
      "Epoch 194/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5708\n",
      "Epoch 195/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5769\n",
      "Epoch 196/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5671\n",
      "Epoch 197/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5495\n",
      "Epoch 198/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6151\n",
      "Epoch 199/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.6052\n",
      "Epoch 200/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5634\n",
      "Epoch 201/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5766\n",
      "Epoch 202/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5499\n",
      "Epoch 203/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5457\n",
      "Epoch 204/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5529\n",
      "Epoch 205/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5450\n",
      "Epoch 206/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5627\n",
      "Epoch 207/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5750\n",
      "Epoch 208/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5471\n",
      "Epoch 209/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5575\n",
      "Epoch 210/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5702\n",
      "Epoch 211/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5607\n",
      "Epoch 212/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5538\n",
      "Epoch 213/256\n",
      "107270/107270 [==============================] - 6s 58us/step - loss: 0.5727\n",
      "Epoch 214/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5505\n",
      "Epoch 215/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5422\n",
      "Epoch 216/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5437\n",
      "Epoch 217/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5477\n",
      "Epoch 218/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5570\n",
      "Epoch 219/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5396\n",
      "Epoch 220/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5546\n",
      "Epoch 221/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5619\n",
      "Epoch 222/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5372\n",
      "Epoch 223/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5499\n",
      "Epoch 224/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5502\n",
      "Epoch 225/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5486\n",
      "Epoch 226/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5628\n",
      "Epoch 227/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5468\n",
      "Epoch 228/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5521\n",
      "Epoch 229/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5557\n",
      "Epoch 230/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5350\n",
      "Epoch 231/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5535\n",
      "Epoch 232/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5339\n",
      "Epoch 233/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5428\n",
      "Epoch 234/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5619\n",
      "Epoch 235/256\n",
      "107270/107270 [==============================] - 7s 64us/step - loss: 0.5387\n",
      "Epoch 236/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5598\n",
      "Epoch 237/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5649\n",
      "Epoch 238/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5506\n",
      "Epoch 239/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5668\n",
      "Epoch 240/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5412\n",
      "Epoch 241/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5753\n",
      "Epoch 242/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5491\n",
      "Epoch 243/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5316\n",
      "Epoch 244/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5414\n",
      "Epoch 245/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5514\n",
      "Epoch 246/256\n",
      "107270/107270 [==============================] - 7s 65us/step - loss: 0.5907\n",
      "Epoch 247/256\n",
      "107270/107270 [==============================] - 6s 59us/step - loss: 0.5435\n",
      "Epoch 248/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5321\n",
      "Epoch 249/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5469\n",
      "Epoch 250/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5677\n",
      "Epoch 251/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5819\n",
      "Epoch 252/256\n",
      "107270/107270 [==============================] - 7s 63us/step - loss: 0.5653\n",
      "Epoch 253/256\n",
      "107270/107270 [==============================] - 6s 61us/step - loss: 0.5490\n",
      "Epoch 254/256\n",
      "107270/107270 [==============================] - 6s 60us/step - loss: 0.5467\n",
      "Epoch 255/256\n",
      "107270/107270 [==============================] - 7s 62us/step - loss: 0.5426\n",
      "Epoch 256/256\n",
      "107270/107270 [==============================] - 7s 61us/step - loss: 0.5570\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CharacterLanguageModelVectorizer(context_length=5, maximum_ordinal=128)\n",
    "model = RecurrentLanguageModel(vectorizer, gpu_optimized=True)\n",
    "with open('cities.txt', encoding='utf8') as cities:\n",
    "    model.fit(cities.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model converges relatively quickly -- more data or more training could be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the interesting part -- generation. This uses a random seed to kick things off, and then pulls out characters until we hit a terminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLanguageModelGenerator():\n",
    "    '''\n",
    "    Given a language model, generate new name strings given a seed of your own design.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, language_model):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        language_model\n",
    "            A trained language model used to generate predictions.\n",
    "        '''\n",
    "        self.language_model = language_model\n",
    "        \n",
    "    def generate(self, seed, max_length=32):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : str\n",
    "            A string to bootstrap generation.\n",
    "        max_length: int\n",
    "            A guard value to prevent looping forever.\n",
    "        '''\n",
    "        \n",
    "        for i in range(0, max_length):\n",
    "            # working on the right most context\n",
    "            X, _ = self.language_model.vectorizer.transform([seed])\n",
    "            context = np.array([X[-1]])\n",
    "            # only need the very first sample, then keep iterating\n",
    "            try:\n",
    "                prediction = self.language_model.model.predict(context)[0]\n",
    "                next_character = self.language_model.vectorizer.inverse_transform(prediction)\n",
    "            except IndexError:\n",
    "                # when we hit a null character, it is time to exit\n",
    "                break\n",
    "            # keep expanding the seed with each word\n",
    "            seed += next_character\n",
    "        return seed.strip().title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sense, this is a kind of a machine learning made up autocomplete, we'll start with a few characters and see what it tacks on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorizer = CharacterLanguageModelVectorizer(context_length=5, maximum_ordinal=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Holstein'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NameLanguageModelGenerator(model).generate('New ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smouth'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NameLanguageModelGenerator(model).generate('Sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smead'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NameLanguageModelGenerator(model).generate('Sme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smanley'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NameLanguageModelGenerator(model).generate('Smanl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
