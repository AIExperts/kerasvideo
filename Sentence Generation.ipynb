{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for generating sentences, starting with a language model. This differs in that it works on words -- with embeddings -- rather than on individual characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class WordLanguageModelVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Base language model uses a CharacterEncoder to create character ordinals\n",
    "    and then applies a transformation in order to create vectors.\n",
    "    '''\n",
    "    def __init__(self, context_length=64):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of words will be used as a context to predict future words.\n",
    "        '''\n",
    "        self.context_length = context_length\n",
    "        self.sequencer = CountVectorizer()\n",
    "    \n",
    "    def fit(self, strings):\n",
    "        '''\n",
    "        Fit the word vocabulary to target strings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "        '''\n",
    "        # forgive passing a single string\n",
    "        if type(strings) is str:\n",
    "            strings = [strings]\n",
    "        self.sequencer.fit(strings)\n",
    "        self.sequencer.inverse_vocabulary_ = {sequence: word for word, sequence in self.sequencer.vocabulary_.items()}\n",
    "        self.unique_words = len(self.sequencer.inverse_vocabulary_)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a (X, Y) pairing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) three dimensional [sample_index, character_index] context X with a word sequence number\n",
    "            to be embedded, and a two dimensional [sample_index, one_hot] target Y.\n",
    "        '''\n",
    "        # forgive passing a single string\n",
    "        if type(strings) is str:\n",
    "            strings = [strings]\n",
    "        # start off by turning all the text into a series of integers\n",
    "        word_sequence_numbers = []\n",
    "        for string in strings:\n",
    "            as_words = self.sequencer.build_analyzer()(string)\n",
    "            word_sequence_numbers += list(map(self.sequencer.vocabulary_.get, as_words))\n",
    "        # pad to the minimum context length\n",
    "        if len(word_sequence_numbers) <= self.context_length:\n",
    "            word_sequence_numbers = [0] * (1 + self.context_length - len(word_sequence_numbers)) + word_sequence_numbers\n",
    "            \n",
    "        # make this number of overlappinq sequences\n",
    "        # ex with context 2: The quick brown fox likes chickens\n",
    "        # The quick -> brown\n",
    "        # quick brown -> fox\n",
    "        number_of_contexts = len(word_sequence_numbers) - self.context_length\n",
    "        # sequence numbers for context words\n",
    "        x = np.zeros((number_of_contexts, self.context_length), dtype=np.int32)\n",
    "        # one hot encodings for target words\n",
    "        y = np.zeros((number_of_contexts, self.unique_words), dtype=np.bool)\n",
    "        for i in range(number_of_contexts):\n",
    "            context = np.array(word_sequence_numbers[i:i+self.context_length])\n",
    "            x[i] = context\n",
    "            target = word_sequence_numbers[i+self.context_length]\n",
    "            y[i, target] = True\n",
    "        return x, y\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of one hot encodings, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        ordinals = X.argmax(-1)\n",
    "        decoder = np.vectorize(self.sequencer.inverse_vocabulary_.get)\n",
    "        # allow for single words or lists of words\n",
    "        decoded = np.array([decoder(ordinals)])\n",
    "        return ' '.join(decoded.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = WordLanguageModelVectorizer()\n",
    "with open('the_adventures_of_tom_sawyer.txt', encoding='utf8') as books:\n",
    "    X, Y = vectorizer.fit_transform(books.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the encoding as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6770, 5133, 3068, 2111, 4580, 6770,  233, 4580, 6910, 5712, 1334,\n",
       "         971, 4095, 7078, 5684, 1219, 6801, 2111, 3609, 2656, 6770, 7221,\n",
       "        4580,  369,  374,  477, 4472, 1487,  341, 7598,  299, 4472, 5498,\n",
       "        7487, 7729, 4126, 1470, 3616, 2884, 3616,  533, 4623, 5303, 7221,\n",
       "        3616, 7124, 6770, 6746, 4580, 6770, 5133, 3068, 3890, 3465, 7598,\n",
       "        6801, 2111, 4623, 4605,  477, 7675, 3068, 4443, 6885], dtype=int32),\n",
       " array([False, False, False, ..., False, False, False], dtype=bool),\n",
       " 6770)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0], Y[0].argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And -- inverse transformation, turning one-hots back into words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the adventures of tom sawyer complete author mark twain samuel clemens release date august 20 2006 ebook 74 last updated june 2017 language english character set encoding utf start of this project gutenberg ebook tom sawyer produced by david widger the adventures of tom sawyer by mark twain samuel langhorne clemens contents chapter tom aunt polly decides upon her duty tom practices music the challenge private entrance chapter ii strong temptations strategic movements the innocents beguiled chapter iii tom as general triumph and reward dismal felicity commission and omission chapter iv mental acrobatics attending sunday school the superintendent showing off'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(Y[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build up a recurrent neural network to learn a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wballard/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, CuDNNLSTM, Dropout, Dense, Reshape, BatchNormalization, Embedding\n",
    "\n",
    "class EmbeddedRecurrentLanguageModel(BaseEstimator):\n",
    "    '''\n",
    "    Create a language model with a neural network and normalized character encoding.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vectorizer, hidden_layers=256, gpu_optimized=False):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        vectorizer : transformer\n",
    "            Object to transform input strings into numerical encodings.\n",
    "        hidden_layers : int\n",
    "            Size of the model's hidden layer, controls complexity.\n",
    "        gpu_optimized : bool\n",
    "            If True, use special code in keras to boost performance.\n",
    "        '''\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.gpu_optimized = gpu_optimized\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def fit(self, strings, epochs=256, batch_size=256):\n",
    "        '''\n",
    "        Create and fit a model to the passed in strings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable source of string text.\n",
    "        '''\n",
    "        if self.gpu_optimized:\n",
    "            RNN = CuDNNLSTM\n",
    "        else:\n",
    "            RNN = LSTM\n",
    "        X, Y = self.vectorizer.fit_transform(strings)\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.model = model = Sequential()\n",
    "        # begin by embedding character positions\n",
    "        model.add(Embedding(self.vectorizer.unique_words, self.hidden_layers, input_shape=(X.shape[1],)))\n",
    "        # and then work on the embeddings recurrently\n",
    "        model.add(RNN(self.hidden_layers, return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(RNN(self.hidden_layers))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.hidden_layers, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.hidden_layers, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this model with cities. I'm using a GPU - and very much recommend you do so! You can set the optimization to False if you need to use a CPU.\n",
    "\n",
    "City names aren't sentences, so we need to use a relatively short context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "72635/72635 [==============================] - 298s 4ms/step - loss: 7.3194\n",
      "Epoch 2/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 5.8302\n",
      "Epoch 3/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 5.3088\n",
      "Epoch 4/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 4.8433\n",
      "Epoch 5/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 4.3602\n",
      "Epoch 6/256\n",
      "72635/72635 [==============================] - 17s 241us/step - loss: 3.8395\n",
      "Epoch 7/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 3.3049\n",
      "Epoch 8/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 2.7772\n",
      "Epoch 9/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 2.2998\n",
      "Epoch 10/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 1.8813\n",
      "Epoch 11/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 1.5132\n",
      "Epoch 12/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 1.2108\n",
      "Epoch 13/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.9537\n",
      "Epoch 14/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.7434\n",
      "Epoch 15/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.5752\n",
      "Epoch 16/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.4554\n",
      "Epoch 17/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.3681\n",
      "Epoch 18/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.2911\n",
      "Epoch 19/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.2450\n",
      "Epoch 20/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.2179\n",
      "Epoch 21/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.2168\n",
      "Epoch 22/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.2422\n",
      "Epoch 23/256\n",
      "72635/72635 [==============================] - 18s 247us/step - loss: 0.2542\n",
      "Epoch 24/256\n",
      "72635/72635 [==============================] - 18s 246us/step - loss: 0.2267\n",
      "Epoch 25/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.1844\n",
      "Epoch 26/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.1390\n",
      "Epoch 27/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.1173\n",
      "Epoch 28/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.1194\n",
      "Epoch 29/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.1674\n",
      "Epoch 30/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.2124\n",
      "Epoch 31/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.2087\n",
      "Epoch 32/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.1591\n",
      "Epoch 33/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.1085\n",
      "Epoch 34/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0768\n",
      "Epoch 35/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.0681\n",
      "Epoch 36/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0763\n",
      "Epoch 37/256\n",
      "72635/72635 [==============================] - 18s 241us/step - loss: 0.1222\n",
      "Epoch 38/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.2429\n",
      "Epoch 39/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.2367\n",
      "Epoch 40/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.1459\n",
      "Epoch 41/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0755\n",
      "Epoch 42/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0425\n",
      "Epoch 43/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0291\n",
      "Epoch 44/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0233\n",
      "Epoch 45/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0405\n",
      "Epoch 46/256\n",
      "72635/72635 [==============================] - 18s 242us/step - loss: 0.2011\n",
      "Epoch 47/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.4129\n",
      "Epoch 48/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.1960\n",
      "Epoch 49/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0718\n",
      "Epoch 50/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0298\n",
      "Epoch 51/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0130\n",
      "Epoch 52/256\n",
      "72635/72635 [==============================] - 18s 246us/step - loss: 0.0080\n",
      "Epoch 53/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0054\n",
      "Epoch 54/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0050\n",
      "Epoch 55/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.0043\n",
      "Epoch 56/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0040\n",
      "Epoch 57/256\n",
      "72635/72635 [==============================] - 18s 243us/step - loss: 0.0060\n",
      "Epoch 58/256\n",
      "72635/72635 [==============================] - 18s 251us/step - loss: 0.4740\n",
      "Epoch 59/256\n",
      "72635/72635 [==============================] - 18s 251us/step - loss: 0.8207\n",
      "Epoch 60/256\n",
      "72635/72635 [==============================] - 18s 249us/step - loss: 0.1511\n",
      "Epoch 61/256\n",
      "72635/72635 [==============================] - 18s 249us/step - loss: 0.0346\n",
      "Epoch 62/256\n",
      "72635/72635 [==============================] - 18s 245us/step - loss: 0.0112\n",
      "Epoch 63/256\n",
      "72635/72635 [==============================] - 18s 241us/step - loss: 0.0050\n",
      "Epoch 64/256\n",
      "72635/72635 [==============================] - 18s 241us/step - loss: 0.0031\n",
      "Epoch 65/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0023\n",
      "Epoch 66/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0018\n",
      "Epoch 67/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0014\n",
      "Epoch 68/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0017\n",
      "Epoch 69/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0015\n",
      "Epoch 70/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0012\n",
      "Epoch 71/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0011\n",
      "Epoch 72/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0016\n",
      "Epoch 73/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.6088\n",
      "Epoch 74/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.8398\n",
      "Epoch 75/256\n",
      "72635/72635 [==============================] - 17s 240us/step - loss: 0.1282\n",
      "Epoch 76/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0274\n",
      "Epoch 77/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0099\n",
      "Epoch 78/256\n",
      "72635/72635 [==============================] - 17s 240us/step - loss: 0.0058\n",
      "Epoch 79/256\n",
      "72635/72635 [==============================] - 18s 244us/step - loss: 0.0042\n",
      "Epoch 80/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0039\n",
      "Epoch 81/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0029\n",
      "Epoch 82/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0031\n",
      "Epoch 83/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0031\n",
      "Epoch 84/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0056\n",
      "Epoch 85/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0370\n",
      "Epoch 86/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.7010\n",
      "Epoch 87/256\n",
      "72635/72635 [==============================] - 17s 240us/step - loss: 0.2432\n",
      "Epoch 88/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0564\n",
      "Epoch 89/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0160\n",
      "Epoch 90/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0061\n",
      "Epoch 91/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0040\n",
      "Epoch 92/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0025\n",
      "Epoch 93/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0019\n",
      "Epoch 94/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0015\n",
      "Epoch 95/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0013\n",
      "Epoch 96/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0012\n",
      "Epoch 97/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0015\n",
      "Epoch 98/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0019\n",
      "Epoch 99/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0804\n",
      "Epoch 100/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.8513\n",
      "Epoch 101/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.1681\n",
      "Epoch 102/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0332\n",
      "Epoch 103/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0093\n",
      "Epoch 104/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0041\n",
      "Epoch 105/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0025\n",
      "Epoch 106/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0019\n",
      "Epoch 107/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0015\n",
      "Epoch 108/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0013\n",
      "Epoch 109/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0011\n",
      "Epoch 110/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0011\n",
      "Epoch 111/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 9.6529e-04\n",
      "Epoch 112/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 8.5096e-04\n",
      "Epoch 113/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 7.4951e-04\n",
      "Epoch 114/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 6.7852e-04\n",
      "Epoch 115/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 6.0224e-04\n",
      "Epoch 116/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 5.5721e-04\n",
      "Epoch 117/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.2165\n",
      "Epoch 118/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.9554\n",
      "Epoch 119/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.1181\n",
      "Epoch 120/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0221\n",
      "Epoch 121/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0070\n",
      "Epoch 122/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0040\n",
      "Epoch 123/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0028\n",
      "Epoch 124/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0021\n",
      "Epoch 125/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0018\n",
      "Epoch 126/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0015\n",
      "Epoch 127/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0012\n",
      "Epoch 128/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0011\n",
      "Epoch 129/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 9.8705e-04\n",
      "Epoch 130/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 8.8853e-04\n",
      "Epoch 131/256\n",
      "72635/72635 [==============================] - 17s 240us/step - loss: 0.0013\n",
      "Epoch 132/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0137\n",
      "Epoch 133/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.7361\n",
      "Epoch 134/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.1978\n",
      "Epoch 135/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0350\n",
      "Epoch 136/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0098\n",
      "Epoch 137/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 0.0043\n",
      "Epoch 138/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0029\n",
      "Epoch 139/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0020\n",
      "Epoch 140/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 0.0018\n",
      "Epoch 141/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0013\n",
      "Epoch 142/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0014\n",
      "Epoch 143/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0012\n",
      "Epoch 144/256\n",
      "72635/72635 [==============================] - 17s 238us/step - loss: 9.1137e-04\n",
      "Epoch 145/256\n",
      "72635/72635 [==============================] - 17s 239us/step - loss: 8.0345e-04\n",
      "Epoch 146/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 7.2669e-04\n",
      "Epoch 147/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 6.7135e-04\n",
      "Epoch 148/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 6.2541e-04\n",
      "Epoch 149/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 7.6889e-04\n",
      "Epoch 150/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.4921\n",
      "Epoch 151/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.4553\n",
      "Epoch 152/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0666\n",
      "Epoch 153/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0136\n",
      "Epoch 154/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0053\n",
      "Epoch 155/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0032\n",
      "Epoch 156/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0021\n",
      "Epoch 157/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0017\n",
      "Epoch 158/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0014\n",
      "Epoch 159/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0016\n",
      "Epoch 160/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0016\n",
      "Epoch 161/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0013\n",
      "Epoch 162/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0011\n",
      "Epoch 163/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0017\n",
      "Epoch 164/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0419\n",
      "Epoch 165/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.5519\n",
      "Epoch 166/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.1284\n",
      "Epoch 167/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0284\n",
      "Epoch 168/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0080\n",
      "Epoch 169/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0036\n",
      "Epoch 170/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0026\n",
      "Epoch 171/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0025\n",
      "Epoch 172/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0017\n",
      "Epoch 173/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0013\n",
      "Epoch 174/256\n",
      "72635/72635 [==============================] - 17s 240us/step - loss: 0.0012\n",
      "Epoch 175/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0010\n",
      "Epoch 176/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 9.6964e-04\n",
      "Epoch 177/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 8.9335e-04\n",
      "Epoch 178/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 8.3102e-04\n",
      "Epoch 179/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 7.8766e-04\n",
      "Epoch 180/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 7.2968e-04\n",
      "Epoch 181/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72635/72635 [==============================] - 17s 236us/step - loss: 7.0467e-04\n",
      "Epoch 182/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 6.6953e-04\n",
      "Epoch 183/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.4749e-04\n",
      "Epoch 184/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.2605e-04\n",
      "Epoch 185/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.0193e-04\n",
      "Epoch 186/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 5.8364e-04\n",
      "Epoch 187/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 5.6708e-04\n",
      "Epoch 188/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 5.5415e-04\n",
      "Epoch 189/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.7499\n",
      "Epoch 190/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.3239\n",
      "Epoch 191/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0442\n",
      "Epoch 192/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0107\n",
      "Epoch 193/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0047\n",
      "Epoch 194/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0029\n",
      "Epoch 195/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 0.0022\n",
      "Epoch 196/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0022\n",
      "Epoch 197/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0022\n",
      "Epoch 198/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0020\n",
      "Epoch 199/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0017\n",
      "Epoch 200/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0016\n",
      "Epoch 201/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0018\n",
      "Epoch 202/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0021\n",
      "Epoch 203/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0027\n",
      "Epoch 204/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.1344\n",
      "Epoch 205/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 0.4296\n",
      "Epoch 206/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0837\n",
      "Epoch 207/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0190\n",
      "Epoch 208/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0061\n",
      "Epoch 209/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0030\n",
      "Epoch 210/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0018\n",
      "Epoch 211/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0014\n",
      "Epoch 212/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0011\n",
      "Epoch 213/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 9.8649e-04\n",
      "Epoch 214/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 8.7154e-04\n",
      "Epoch 215/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 7.6044e-04\n",
      "Epoch 216/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.7849e-04\n",
      "Epoch 217/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.1639e-04\n",
      "Epoch 218/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 5.5585e-04\n",
      "Epoch 219/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 7.6019e-04\n",
      "Epoch 220/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.2366\n",
      "Epoch 221/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.3976\n",
      "Epoch 222/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0632\n",
      "Epoch 223/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0143\n",
      "Epoch 224/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0050\n",
      "Epoch 225/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0027\n",
      "Epoch 226/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0021\n",
      "Epoch 227/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0014\n",
      "Epoch 228/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0011\n",
      "Epoch 229/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 9.1313e-04\n",
      "Epoch 230/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 8.2400e-04\n",
      "Epoch 231/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 7.3431e-04\n",
      "Epoch 232/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.6446e-04\n",
      "Epoch 233/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 6.0755e-04\n",
      "Epoch 234/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 5.5940e-04\n",
      "Epoch 235/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 5.1610e-04\n",
      "Epoch 236/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 9.3828e-04\n",
      "Epoch 237/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 0.3372\n",
      "Epoch 238/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.3114\n",
      "Epoch 239/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0484\n",
      "Epoch 240/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0103\n",
      "Epoch 241/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0040\n",
      "Epoch 242/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0022\n",
      "Epoch 243/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0021\n",
      "Epoch 244/256\n",
      "72635/72635 [==============================] - 17s 234us/step - loss: 0.0017\n",
      "Epoch 245/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0013\n",
      "Epoch 246/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0013\n",
      "Epoch 247/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0010\n",
      "Epoch 248/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0013\n",
      "Epoch 249/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0013\n",
      "Epoch 250/256\n",
      "72635/72635 [==============================] - 17s 237us/step - loss: 0.0022\n",
      "Epoch 251/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0047\n",
      "Epoch 252/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.3174\n",
      "Epoch 253/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.1830\n",
      "Epoch 254/256\n",
      "72635/72635 [==============================] - 17s 235us/step - loss: 0.0363\n",
      "Epoch 255/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0091\n",
      "Epoch 256/256\n",
      "72635/72635 [==============================] - 17s 236us/step - loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "vectorizer = WordLanguageModelVectorizer()\n",
    "model = EmbeddedRecurrentLanguageModel(vectorizer, gpu_optimized=True)\n",
    "with open('the_adventures_of_tom_sawyer.txt', encoding='utf8') as books:\n",
    "    model.fit(books.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectorizer = WordLanguageModelVectorizer()\n",
    "with open('the_adventures_of_tom_sawyer.txt', encoding='utf8') as books:\n",
    "    X, Y = model.vectorizer.fit_transform(books.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for generation. Any words you like -- which will be padded as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLanguageModelGenerator():\n",
    "    '''\n",
    "    Given a language model, generate new name strings given a seed of your own design.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, language_model):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        language_model\n",
    "            A trained language model used to generate predictions.\n",
    "        '''\n",
    "        self.language_model = language_model\n",
    "        \n",
    "    def generate(self, seed, max_length=64):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : str\n",
    "            A string to bootstrap generation.\n",
    "        max_length: int\n",
    "            A guard value to prevent looping forever.\n",
    "        '''\n",
    "       \n",
    "        # build up the result buffer here, adding on to our passed seed\n",
    "        result = seed\n",
    "        for i in range(0, max_length):\n",
    "            # working on the right most context\n",
    "            X, _ = self.language_model.vectorizer.transform([seed])\n",
    "            context = np.array([X[-1]])\n",
    "            # only need the very first sample, then keep iterating\n",
    "            try:\n",
    "                prediction = self.language_model.model.predict(context)[0]\n",
    "                next_word = self.language_model.vectorizer.inverse_transform(prediction)\n",
    "            except IndexError:\n",
    "                # when we hit a null character, it is time to exit\n",
    "                break\n",
    "            # keep expanding the seed with each word\n",
    "            seed += ' ' + next_word\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sense, this is a kind of a machine learning made up autocomplete, we'll start with a few words and see what it tacks on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like to understand all of the ways country summer mighty right grave in in our front secret of for injun ever joe have harper been always away built he or another stop kite tom strings had stick presence to with work one of sort them of men men were were gone in and the late afternoon tavern after and breakfast they although presently the the book welshman almost and went they'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceLanguageModelGenerator(model).generate('I would like to understand all of the ways')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
