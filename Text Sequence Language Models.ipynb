{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning long strings of text into machine learning models for prediction is all about predicting the next character from a context of previous characters.\n",
    "\n",
    "Given a string, and a context window length, a string is transformed like this:\n",
    "\n",
    "'Hello world!', 5\n",
    "\n",
    "'Hello' -> ' '\n",
    "'ello ' -> 'w'\n",
    "'llo w' -> 'o'\n",
    "\n",
    "So the first order of business is building a transformer to build strings in such a fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "class CharacterEncoder(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transform a string into context and target character sequence numbers, using the ordinal\n",
    "    value of each character.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.context_length = context_length\n",
    "        self.maximum_ordinal = maximum_ordinal\n",
    "        \n",
    "    def fit(self, strings, **kwargs):\n",
    "        '''\n",
    "        No need to fit.\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform an iterable source of strings into a dense matrix\n",
    "        of character identifiers.\n",
    "        \n",
    "        Each sample will be a string snippet of context_length characters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character identifier, and\n",
    "            a one dimensional [sample_index] with a 32 bit character identifier to predict.\n",
    "        '''\n",
    "        # forgive passing a single string\n",
    "        if type(strings) is str:\n",
    "            strings = [strings]\n",
    "        # buffer up contexts and targets, we'll be pridicting target characters\n",
    "        # from context strings\n",
    "        contexts = []\n",
    "        targets = []\n",
    "        for i, string in enumerate(strings):\n",
    "            # lowercase and stripped of leading whitespace, makes the model more compact\n",
    "            string = string.lower().strip()\n",
    "            # null character termination for each string\n",
    "            string += chr(0)\n",
    "            for j in range(0, len(string) - self.context_length):\n",
    "                contexts.append(string[j:j + self.context_length])\n",
    "                targets.append(string[j + self.context_length])\n",
    "        # blocks of memory to hold character ordinals\n",
    "        X = np.zeros((len(contexts), self.context_length), dtype=np.int32)\n",
    "        Y = np.zeros(len(targets), dtype=np.int32)\n",
    "        # numerical encoding of character values\n",
    "        for i, context in enumerate(contexts):\n",
    "            for j, character in enumerate(context):\n",
    "                X[i, j] = min(ord(character), self.maximum_ordinal)\n",
    "        for i, character in enumerate(targets):\n",
    "            Y[i] = min(ord(character), self.maximum_ordinal)\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[116, 114,  97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105],\n",
       "        [114,  97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110],\n",
       "        [ 97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103],\n",
       "        [110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32],\n",
       "        [115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105],\n",
       "        [102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110],\n",
       "        [111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116],\n",
       "        [114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111],\n",
       "        [109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32],\n",
       "        [ 32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99],\n",
       "        [ 97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111],\n",
       "        [ 32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110],\n",
       "        [115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116],\n",
       "        [116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101],\n",
       "        [114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120],\n",
       "        [105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116],\n",
       "        [110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32],\n",
       "        [103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97],\n",
       "        [ 32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110],\n",
       "        [105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100],\n",
       "        [110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32],\n",
       "        [116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116],\n",
       "        [111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97],\n",
       "        [ 32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114],\n",
       "        [ 99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103],\n",
       "        [111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101],\n",
       "        [110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116],\n",
       "        [116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32],\n",
       "        [101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99],\n",
       "        [120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104],\n",
       "        [116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97],\n",
       "        [ 32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114],\n",
       "        [ 97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97],\n",
       "        [110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99],\n",
       "        [100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116],\n",
       "        [ 32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101],\n",
       "        [116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114],\n",
       "        [ 97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32],\n",
       "        [114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115],\n",
       "        [103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101],\n",
       "        [101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113],\n",
       "        [116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117],\n",
       "        [ 32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101],\n",
       "        [ 99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110],\n",
       "        [104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99],\n",
       "        [ 97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101],\n",
       "        [114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32],\n",
       "        [ 97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110],\n",
       "        [ 99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117],\n",
       "        [116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109],\n",
       "        [101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98],\n",
       "        [114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101],\n",
       "        [ 32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114],\n",
       "        [115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114, 115],\n",
       "        [101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114, 115,  46]], dtype=int32),\n",
       " array([110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110,\n",
       "        100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,\n",
       "         32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114, 115,  46,   0], dtype=int32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=100)\n",
    "CharacterEncoder().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sequences in hand -- we have two basic approaches to vectorizing:\n",
    "* Normalization, which uses the same amount of memory\n",
    "* One Hot, which makes another dimension in the matrix, using more memory\n",
    "\n",
    "In this encoder, we will be using both -- normalized inputs and one hot encoded outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Base language model uses a CharacterEncoder to create character ordinals\n",
    "    and then applies a transformation in order to create vectors.\n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.sequencer = CharacterEncoder(context_length, maximum_ordinal)\n",
    "    \n",
    "    def fit(self, strings):\n",
    "        '''\n",
    "        Nothing to fit.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a dense (X, Y) pairing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character encoding, and\n",
    "            a one dimensional [sample_index] with a 32 bit character encoding to predict.\n",
    "        '''\n",
    "        # character sequence numbers\n",
    "        X, Y = self.sequencer.transform(strings)\n",
    "        # one hot context encoding\n",
    "        x = np.zeros((X.shape[0], self.sequencer.context_length, self.sequencer.maximum_ordinal), dtype=np.bool)\n",
    "        y = np.zeros((Y.shape[0], self.sequencer.maximum_ordinal), dtype=np.bool)\n",
    "        for i, context in enumerate(X):\n",
    "            for t, char in enumerate(context):\n",
    "                x[i, t, char] = 1\n",
    "        for i, target in enumerate(Y):\n",
    "            y[i, target] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of one hot encodings, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        ordinals = X.argmax(-1).astype(np.int32)\n",
    "        decode = np.vectorize(chr)\n",
    "        return ''.join(decode(ordinals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First with the normalized model, this is as simple as division by the max character number seen. It uses no more memory than sequence numbers, the trick is reading back predictions -- which will be floating point, with the need to round to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = LanguageModelVectorizer().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       " \n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       " \n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       " \n",
       "        ..., \n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       " \n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       " \n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ..., \n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]], dtype=bool),\n",
       " array([[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ..., \n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [ True, False, False, ..., False, False, False]], dtype=bool))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot outputs will eventually be used to turn back into strings, let's make sure we can get characters from one-hots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ng into context and target character sequence numbers.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LanguageModelVectorizer().inverse_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much memory we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61276160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes + Y.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, and alternate approach to encoding -- normalization to create a dense character encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedLanguageModelVectorizer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Create a language model with normalized representations of characters on the\n",
    "    range of 0-1. Not a one hot encoding, but a dense encoding of character values.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.sequencer = CharacterEncoder(context_length, maximum_ordinal)\n",
    "    \n",
    "    def fit(self, strings):\n",
    "        '''\n",
    "        Nothing to fit.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a dense (X, Y) pairing\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character encoding, and\n",
    "            a one dimensional [sample_index] with a 32 bit character encoding to predict.\n",
    "        '''\n",
    "        X, Y = self.sequencer.transform(strings)\n",
    "        return X / self.sequencer.maximum_ordinal, Y / self.sequencer.maximum_ordinal\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of numbers, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        scaled = X * self.sequencer.maximum_ordinal\n",
    "        ordinals = scaled.round().astype(np.int32)\n",
    "        decode = np.vectorize(chr)\n",
    "        return ''.join(decode(ordinals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = NormalizedLanguageModelVectorizer().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00177002,  0.0017395 ,  0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,\n",
       "          0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,\n",
       "          0.0017395 ,  0.00160217],\n",
       "        [ 0.0017395 ,  0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,\n",
       "          0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,\n",
       "          0.00160217,  0.00167847],\n",
       "        [ 0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,\n",
       "          0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,\n",
       "          0.00167847,  0.00157166],\n",
       "        [ 0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,\n",
       "          0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,\n",
       "          0.00157166,  0.00048828],\n",
       "        [ 0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,\n",
       "          0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,\n",
       "          0.00048828,  0.00160217],\n",
       "        [ 0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,\n",
       "          0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,\n",
       "          0.00160217,  0.00167847],\n",
       "        [ 0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,\n",
       "          0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,\n",
       "          0.00167847,  0.00177002],\n",
       "        [ 0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,\n",
       "          0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,\n",
       "          0.00177002,  0.00169373],\n",
       "        [ 0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,\n",
       "          0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,\n",
       "          0.00169373,  0.00048828],\n",
       "        [ 0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,\n",
       "          0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "          0.00048828,  0.00151062],\n",
       "        [ 0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,\n",
       "          0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,\n",
       "          0.00151062,  0.00169373],\n",
       "        [ 0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,\n",
       "          0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,\n",
       "          0.00169373,  0.00167847],\n",
       "        [ 0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,\n",
       "          0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,\n",
       "          0.00167847,  0.00177002],\n",
       "        [ 0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,\n",
       "          0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,\n",
       "          0.00177002,  0.00154114],\n",
       "        [ 0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,\n",
       "          0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,\n",
       "          0.00154114,  0.00183105],\n",
       "        [ 0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,\n",
       "          0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,\n",
       "          0.00183105,  0.00177002],\n",
       "        [ 0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "          0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "          0.00177002,  0.00048828],\n",
       "        [ 0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,\n",
       "          0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,\n",
       "          0.00048828,  0.0014801 ],\n",
       "        [ 0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,\n",
       "          0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,\n",
       "          0.0014801 ,  0.00167847],\n",
       "        [ 0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,\n",
       "          0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,\n",
       "          0.00167847,  0.00152588],\n",
       "        [ 0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,\n",
       "          0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,\n",
       "          0.00152588,  0.00048828],\n",
       "        [ 0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,\n",
       "          0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,\n",
       "          0.00048828,  0.00177002],\n",
       "        [ 0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,\n",
       "          0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,\n",
       "          0.00177002,  0.0014801 ],\n",
       "        [ 0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "          0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "          0.0014801 ,  0.0017395 ],\n",
       "        [ 0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,\n",
       "          0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,\n",
       "          0.0017395 ,  0.00157166],\n",
       "        [ 0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,\n",
       "          0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,\n",
       "          0.00157166,  0.00154114],\n",
       "        [ 0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,\n",
       "          0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,\n",
       "          0.00154114,  0.00177002],\n",
       "        [ 0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,\n",
       "          0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,\n",
       "          0.00177002,  0.00048828],\n",
       "        [ 0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,\n",
       "          0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,\n",
       "          0.00048828,  0.00151062],\n",
       "        [ 0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,\n",
       "          0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,\n",
       "          0.00151062,  0.00158691],\n",
       "        [ 0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "          0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "          0.00158691,  0.0014801 ],\n",
       "        [ 0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,\n",
       "          0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,\n",
       "          0.0014801 ,  0.0017395 ],\n",
       "        [ 0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,\n",
       "          0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,\n",
       "          0.0017395 ,  0.0014801 ],\n",
       "        [ 0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,\n",
       "          0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,\n",
       "          0.0014801 ,  0.00151062],\n",
       "        [ 0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,\n",
       "          0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,\n",
       "          0.00151062,  0.00177002],\n",
       "        [ 0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,\n",
       "          0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,\n",
       "          0.00177002,  0.00154114],\n",
       "        [ 0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,\n",
       "          0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,\n",
       "          0.00154114,  0.0017395 ],\n",
       "        [ 0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "          0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "          0.0017395 ,  0.00048828],\n",
       "        [ 0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,\n",
       "          0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,\n",
       "          0.00048828,  0.00175476],\n",
       "        [ 0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,\n",
       "          0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,\n",
       "          0.00175476,  0.00154114],\n",
       "        [ 0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,\n",
       "          0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,\n",
       "          0.00154114,  0.00172424],\n",
       "        [ 0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,\n",
       "          0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,\n",
       "          0.00172424,  0.00178528],\n",
       "        [ 0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,\n",
       "          0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,\n",
       "          0.00178528,  0.00154114],\n",
       "        [ 0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,\n",
       "          0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,\n",
       "          0.00154114,  0.00167847],\n",
       "        [ 0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "          0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "          0.00167847,  0.00151062],\n",
       "        [ 0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,\n",
       "          0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,\n",
       "          0.00151062,  0.00154114],\n",
       "        [ 0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,\n",
       "          0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,\n",
       "          0.00154114,  0.00048828],\n",
       "        [ 0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,\n",
       "          0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,\n",
       "          0.00048828,  0.00167847],\n",
       "        [ 0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,\n",
       "          0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,\n",
       "          0.00167847,  0.00178528],\n",
       "        [ 0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,\n",
       "          0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,\n",
       "          0.00178528,  0.00166321],\n",
       "        [ 0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,\n",
       "          0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,\n",
       "          0.00166321,  0.00149536],\n",
       "        [ 0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "          0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,\n",
       "          0.00149536,  0.00154114],\n",
       "        [ 0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,\n",
       "          0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,  0.00149536,\n",
       "          0.00154114,  0.0017395 ],\n",
       "        [ 0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,\n",
       "          0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,  0.00149536,  0.00154114,\n",
       "          0.0017395 ,  0.00175476],\n",
       "        [ 0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,\n",
       "          0.00048828,  0.00167847,  0.00178528,  0.00166321,  0.00149536,  0.00154114,  0.0017395 ,\n",
       "          0.00175476,  0.0007019 ]]),\n",
       " array([ 0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "         0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "         0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "         0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "         0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "         0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "         0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,\n",
       "         0.00149536,  0.00154114,  0.0017395 ,  0.00175476,  0.0007019 ,  0.        ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ng into context and target character sequence numbers.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizedLanguageModelVectorizer().inverse_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again a look at the memory used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.nbytes + Y.nbytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
