{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning long strings of text into machine learning models for prediction is all about predicting the next character from a context of previous characters.\n",
    "\n",
    "Given a string, and a context window length, a string is transformed like this:\n",
    "\n",
    "'Hello world!', 5\n",
    "\n",
    "'Hello' -> ' '\n",
    "'ello ' -> 'w'\n",
    "'llo w' -> 'o'\n",
    "\n",
    "So the first order of business is building a transformer to build strings in such a fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import html\n",
    "import numpy as np\n",
    "\n",
    "class LanguageModelSequencer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transform a string into context and target character sequence numbers, using the ordinal\n",
    "    value of each character.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.context_length = context_length\n",
    "        self.maximum_ordinal = maximum_ordinal\n",
    "        # delegate actual parsing to scikit-learn\n",
    "        self.wordbreaker = CountVectorizer(lowercase=True, ngram_range=(1,1), analyzer='char').build_analyzer()\n",
    "        \n",
    "    def fit(self, strings, **kwargs):\n",
    "        '''\n",
    "        No need to fit\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform an iterable source of strings into a dense matrix\n",
    "        of character identifiers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character identifier, and\n",
    "            a one dimensional [sample_index] with a 32 bit character identifier to predict.\n",
    "        '''\n",
    "        # forgive passing a single string\n",
    "        if type(strings) is str:\n",
    "            strings = [strings]\n",
    "        contexts = []\n",
    "        targets = []\n",
    "        for i, string in enumerate(strings):\n",
    "            for j in range(0, len(string) - self.context_length):\n",
    "                contexts.append(string[j:j + self.context_length])\n",
    "                targets.append(string[j + self.context_length])\n",
    "        # blocks of memory to hold character ordinals\n",
    "        X = np.zeros((len(contexts), self.context_length), dtype=np.int32)\n",
    "        Y = np.zeros(len(targets), dtype=np.int32)\n",
    "        for i, context in enumerate(contexts):\n",
    "            for j, character in enumerate(context):\n",
    "                X[i, j] = min(ord(character), self.maximum_ordinal)\n",
    "        for i, character in enumerate(targets):\n",
    "            Y[i] = min(ord(character), self.maximum_ordinal)\n",
    "        return X, Y\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 84, 114,  97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105],\n",
       "        [114,  97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110],\n",
       "        [ 97, 110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103],\n",
       "        [110, 115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32],\n",
       "        [115, 102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105],\n",
       "        [102, 111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110],\n",
       "        [111, 114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116],\n",
       "        [114, 109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111],\n",
       "        [109,  32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32],\n",
       "        [ 32,  97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99],\n",
       "        [ 97,  32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111],\n",
       "        [ 32, 115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110],\n",
       "        [115, 116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116],\n",
       "        [116, 114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101],\n",
       "        [114, 105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120],\n",
       "        [105, 110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116],\n",
       "        [110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32],\n",
       "        [103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97],\n",
       "        [ 32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110],\n",
       "        [105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100],\n",
       "        [110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32],\n",
       "        [116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116],\n",
       "        [111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97],\n",
       "        [ 32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114],\n",
       "        [ 99, 111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103],\n",
       "        [111, 110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101],\n",
       "        [110, 116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116],\n",
       "        [116, 101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32],\n",
       "        [101, 120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99],\n",
       "        [120, 116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104],\n",
       "        [116,  32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97],\n",
       "        [ 32,  97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114],\n",
       "        [ 97, 110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97],\n",
       "        [110, 100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99],\n",
       "        [100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116],\n",
       "        [ 32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101],\n",
       "        [116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114],\n",
       "        [ 97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32],\n",
       "        [114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115],\n",
       "        [103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101],\n",
       "        [101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113],\n",
       "        [116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117],\n",
       "        [ 32,  99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101],\n",
       "        [ 99, 104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110],\n",
       "        [104,  97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99],\n",
       "        [ 97, 114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101],\n",
       "        [114,  97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32],\n",
       "        [ 97,  99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110],\n",
       "        [ 99, 116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117],\n",
       "        [116, 101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109],\n",
       "        [101, 114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98],\n",
       "        [114,  32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101],\n",
       "        [ 32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114],\n",
       "        [115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114, 115]], dtype=int32),\n",
       " array([110, 103,  32, 105, 110, 116, 111,  32,  99, 111, 110, 116, 101, 120, 116,  32,  97, 110,\n",
       "        100,  32, 116,  97, 114, 103, 101, 116,  32,  99, 104,  97, 114,  97,  99, 116, 101, 114,\n",
       "         32, 115, 101, 113, 117, 101, 110,  99, 101,  32, 110, 117, 109,  98, 101, 114, 115,  46], dtype=int32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=100)\n",
    "LanguageModelSequencer().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sequences in hand -- we have two basic approaches to vectorizing:\n",
    "* Normalization, which uses the same amount of memory\n",
    "* One Hot, which makes another dimension in the matrix, using more memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLanguageModel(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Base language model uses a LanguageModelSequencer to create character ordinals\n",
    "    and then applies a final transformation in order to create vectors.\n",
    "    '''\n",
    "    def __init__(self, context_length=16, maximum_ordinal=2**16):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_length : int\n",
    "            This number of characters will be used as a context to predict future characters.\n",
    "        maximum_ordinal : int\n",
    "            Limit total memory use in case you run into very high unicode characters.\n",
    "        '''\n",
    "        self.sequencer = LanguageModelSequencer(context_length=16, maximum_ordinal=maximum_ordinal)\n",
    "    \n",
    "    def fit(self, strings):\n",
    "        '''\n",
    "        Nothing to fit.\n",
    "        '''\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First with the normalized model, this is as simple as division by the max character number seen. It uses no more memory than sequence numbers, the trick is reading back predictions -- which will be floating point, with the need to round to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedLanguageModel(BaseLanguageModel):\n",
    "    '''\n",
    "    Create a language model with normalized representations of characters on the\n",
    "    range of 0-1.\n",
    "    '''\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a dense (X, Y) pairing\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 2 dimensional [sample_index, character], with a 32 bit character encoding, and\n",
    "            a one dimensional [sample_index] with a 32 bit character encoding to predict.\n",
    "        '''\n",
    "        X, Y = self.sequencer.transform(strings)\n",
    "        return X / self.sequencer.maximum_ordinal, Y / self.sequencer.maximum_ordinal\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of numbers, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        scaled = X * self.sequencer.maximum_ordinal\n",
    "        ordinals = scaled.round().astype(np.int32)\n",
    "        decode = np.vectorize(chr)\n",
    "        return decode(ordinals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = NormalizedLanguageModel().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00128174,  0.0017395 ,  0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,\n",
       "          0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,\n",
       "          0.0017395 ,  0.00160217],\n",
       "        [ 0.0017395 ,  0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,\n",
       "          0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,\n",
       "          0.00160217,  0.00167847],\n",
       "        [ 0.0014801 ,  0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,\n",
       "          0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,\n",
       "          0.00167847,  0.00157166],\n",
       "        [ 0.00167847,  0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,\n",
       "          0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,\n",
       "          0.00157166,  0.00048828],\n",
       "        [ 0.00175476,  0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,\n",
       "          0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,\n",
       "          0.00048828,  0.00160217],\n",
       "        [ 0.0015564 ,  0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,\n",
       "          0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,\n",
       "          0.00160217,  0.00167847],\n",
       "        [ 0.00169373,  0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,\n",
       "          0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,\n",
       "          0.00167847,  0.00177002],\n",
       "        [ 0.0017395 ,  0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,\n",
       "          0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,\n",
       "          0.00177002,  0.00169373],\n",
       "        [ 0.00166321,  0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,\n",
       "          0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,\n",
       "          0.00169373,  0.00048828],\n",
       "        [ 0.00048828,  0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,\n",
       "          0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "          0.00048828,  0.00151062],\n",
       "        [ 0.0014801 ,  0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,\n",
       "          0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,\n",
       "          0.00151062,  0.00169373],\n",
       "        [ 0.00048828,  0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,\n",
       "          0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,\n",
       "          0.00169373,  0.00167847],\n",
       "        [ 0.00175476,  0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,\n",
       "          0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,\n",
       "          0.00167847,  0.00177002],\n",
       "        [ 0.00177002,  0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,\n",
       "          0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,\n",
       "          0.00177002,  0.00154114],\n",
       "        [ 0.0017395 ,  0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,\n",
       "          0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,\n",
       "          0.00154114,  0.00183105],\n",
       "        [ 0.00160217,  0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,\n",
       "          0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,\n",
       "          0.00183105,  0.00177002],\n",
       "        [ 0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "          0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "          0.00177002,  0.00048828],\n",
       "        [ 0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,\n",
       "          0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,\n",
       "          0.00048828,  0.0014801 ],\n",
       "        [ 0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,\n",
       "          0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,\n",
       "          0.0014801 ,  0.00167847],\n",
       "        [ 0.00160217,  0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,\n",
       "          0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,\n",
       "          0.00167847,  0.00152588],\n",
       "        [ 0.00167847,  0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,\n",
       "          0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,\n",
       "          0.00152588,  0.00048828],\n",
       "        [ 0.00177002,  0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,\n",
       "          0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,\n",
       "          0.00048828,  0.00177002],\n",
       "        [ 0.00169373,  0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,\n",
       "          0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,\n",
       "          0.00177002,  0.0014801 ],\n",
       "        [ 0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "          0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "          0.0014801 ,  0.0017395 ],\n",
       "        [ 0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,\n",
       "          0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,\n",
       "          0.0017395 ,  0.00157166],\n",
       "        [ 0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,\n",
       "          0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,\n",
       "          0.00157166,  0.00154114],\n",
       "        [ 0.00167847,  0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,\n",
       "          0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,\n",
       "          0.00154114,  0.00177002],\n",
       "        [ 0.00177002,  0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,\n",
       "          0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,\n",
       "          0.00177002,  0.00048828],\n",
       "        [ 0.00154114,  0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,\n",
       "          0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,\n",
       "          0.00048828,  0.00151062],\n",
       "        [ 0.00183105,  0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,\n",
       "          0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,\n",
       "          0.00151062,  0.00158691],\n",
       "        [ 0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "          0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "          0.00158691,  0.0014801 ],\n",
       "        [ 0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,\n",
       "          0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,\n",
       "          0.0014801 ,  0.0017395 ],\n",
       "        [ 0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,\n",
       "          0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,\n",
       "          0.0017395 ,  0.0014801 ],\n",
       "        [ 0.00167847,  0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,\n",
       "          0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,\n",
       "          0.0014801 ,  0.00151062],\n",
       "        [ 0.00152588,  0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,\n",
       "          0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,\n",
       "          0.00151062,  0.00177002],\n",
       "        [ 0.00048828,  0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,\n",
       "          0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,\n",
       "          0.00177002,  0.00154114],\n",
       "        [ 0.00177002,  0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,\n",
       "          0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,\n",
       "          0.00154114,  0.0017395 ],\n",
       "        [ 0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "          0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "          0.0017395 ,  0.00048828],\n",
       "        [ 0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,\n",
       "          0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,\n",
       "          0.00048828,  0.00175476],\n",
       "        [ 0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,\n",
       "          0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,\n",
       "          0.00175476,  0.00154114],\n",
       "        [ 0.00154114,  0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,\n",
       "          0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,\n",
       "          0.00154114,  0.00172424],\n",
       "        [ 0.00177002,  0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,\n",
       "          0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,\n",
       "          0.00172424,  0.00178528],\n",
       "        [ 0.00048828,  0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,\n",
       "          0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,\n",
       "          0.00178528,  0.00154114],\n",
       "        [ 0.00151062,  0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,\n",
       "          0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,\n",
       "          0.00154114,  0.00167847],\n",
       "        [ 0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "          0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "          0.00167847,  0.00151062],\n",
       "        [ 0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,\n",
       "          0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,\n",
       "          0.00151062,  0.00154114],\n",
       "        [ 0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,\n",
       "          0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,\n",
       "          0.00154114,  0.00048828],\n",
       "        [ 0.0014801 ,  0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,\n",
       "          0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,\n",
       "          0.00048828,  0.00167847],\n",
       "        [ 0.00151062,  0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,\n",
       "          0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,\n",
       "          0.00167847,  0.00178528],\n",
       "        [ 0.00177002,  0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,\n",
       "          0.00178528,  0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,\n",
       "          0.00178528,  0.00166321],\n",
       "        [ 0.00154114,  0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,\n",
       "          0.00154114,  0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,\n",
       "          0.00166321,  0.00149536],\n",
       "        [ 0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "          0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,\n",
       "          0.00149536,  0.00154114],\n",
       "        [ 0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,\n",
       "          0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,  0.00149536,\n",
       "          0.00154114,  0.0017395 ],\n",
       "        [ 0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,  0.00167847,  0.00151062,\n",
       "          0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,  0.00149536,  0.00154114,\n",
       "          0.0017395 ,  0.00175476]]),\n",
       " array([ 0.00167847,  0.00157166,  0.00048828,  0.00160217,  0.00167847,  0.00177002,  0.00169373,\n",
       "         0.00048828,  0.00151062,  0.00169373,  0.00167847,  0.00177002,  0.00154114,  0.00183105,\n",
       "         0.00177002,  0.00048828,  0.0014801 ,  0.00167847,  0.00152588,  0.00048828,  0.00177002,\n",
       "         0.0014801 ,  0.0017395 ,  0.00157166,  0.00154114,  0.00177002,  0.00048828,  0.00151062,\n",
       "         0.00158691,  0.0014801 ,  0.0017395 ,  0.0014801 ,  0.00151062,  0.00177002,  0.00154114,\n",
       "         0.0017395 ,  0.00048828,  0.00175476,  0.00154114,  0.00172424,  0.00178528,  0.00154114,\n",
       "         0.00167847,  0.00151062,  0.00154114,  0.00048828,  0.00167847,  0.00178528,  0.00166321,\n",
       "         0.00149536,  0.00154114,  0.0017395 ,  0.00175476,  0.0007019 ]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the inverse transform to reconstruct characters from numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i'],\n",
       "       ['r', 'a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n'],\n",
       "       ['a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g'],\n",
       "       ['n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' '],\n",
       "       ['s', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i'],\n",
       "       ['f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n'],\n",
       "       ['o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't'],\n",
       "       ['r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o'],\n",
       "       ['m', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' '],\n",
       "       [' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c'],\n",
       "       ['a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o'],\n",
       "       [' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n'],\n",
       "       ['s', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't'],\n",
       "       ['t', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e'],\n",
       "       ['r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x'],\n",
       "       ['i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't'],\n",
       "       ['n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' '],\n",
       "       ['g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a'],\n",
       "       [' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n'],\n",
       "       ['i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd'],\n",
       "       ['n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' '],\n",
       "       ['t', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't'],\n",
       "       ['o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a'],\n",
       "       [' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r'],\n",
       "       ['c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g'],\n",
       "       ['o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e'],\n",
       "       ['n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't'],\n",
       "       ['t', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' '],\n",
       "       ['e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c'],\n",
       "       ['x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h'],\n",
       "       ['t', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a'],\n",
       "       [' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r'],\n",
       "       ['a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a'],\n",
       "       ['n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c'],\n",
       "       ['d', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't'],\n",
       "       [' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e'],\n",
       "       ['t', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r'],\n",
       "       ['a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' '],\n",
       "       ['r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's'],\n",
       "       ['g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e'],\n",
       "       ['e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q'],\n",
       "       ['t', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u'],\n",
       "       [' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e'],\n",
       "       ['c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n'],\n",
       "       ['h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c'],\n",
       "       ['a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e'],\n",
       "       ['r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' '],\n",
       "       ['a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n'],\n",
       "       ['c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u'],\n",
       "       ['t', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm'],\n",
       "       ['e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b'],\n",
       "       ['r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e'],\n",
       "       [' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r'],\n",
       "       ['s', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizedLanguageModel().inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n',\n",
       "       'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r',\n",
       "       ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', '.'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizedLanguageModel().inverse_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one hot encoded model. This will use more memory, but the output can simply be like predicting classes with softmax, just pick the character with the largest predicted value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much memory we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes + Y.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wballard/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "class OneHotLanguageModel(BaseLanguageModel):\n",
    "    '''\n",
    "    Create a language model with one hot representations in a character matrix.\n",
    "    '''\n",
    "    \n",
    "    def transform(self, strings):\n",
    "        '''\n",
    "        Transform strings into a dense (X, Y) pairing\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        strings : iterable\n",
    "            An iterable of source strings.\n",
    "       \n",
    "       Returns\n",
    "        -------\n",
    "        (np.ndarray, np.ndarray)\n",
    "            A tuple (X, Y) 3 dimensional [sample_index, character_position, one_hot], \n",
    "            a 2 dimensional [sample_index, one_hot].\n",
    "        '''\n",
    "        X, Y = self.sequencer.transform(strings)\n",
    "        # to_categorical flattens the dimensions, so reshape to our sample/character/one_hot goal\n",
    "        # this is a lot faster than making another loop\n",
    "        X_target_shape = (X.shape[0], X.shape[1], self.sequencer.maximum_ordinal) \n",
    "        X = to_categorical(X, self.sequencer.maximum_ordinal).reshape(X_target_shape)\n",
    "        Y = to_categorical(Y, self.sequencer.maximum_ordinal)\n",
    "        return X, Y\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        '''\n",
    "        Given a matrix of one hot encodings, reverse the transformation and return a matrix of characters.\n",
    "        '''\n",
    "        ordinals = X.argmax(-1).astype(np.int32)\n",
    "        decode = np.vectorize(chr)\n",
    "        return decode(ordinals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = OneHotLanguageModel().fit_transform('Transform a string into context and target character sequence numbers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i'],\n",
       "       ['r', 'a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n'],\n",
       "       ['a', 'n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g'],\n",
       "       ['n', 's', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' '],\n",
       "       ['s', 'f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i'],\n",
       "       ['f', 'o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n'],\n",
       "       ['o', 'r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't'],\n",
       "       ['r', 'm', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o'],\n",
       "       ['m', ' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' '],\n",
       "       [' ', 'a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c'],\n",
       "       ['a', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o'],\n",
       "       [' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n'],\n",
       "       ['s', 't', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't'],\n",
       "       ['t', 'r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e'],\n",
       "       ['r', 'i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x'],\n",
       "       ['i', 'n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't'],\n",
       "       ['n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' '],\n",
       "       ['g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a'],\n",
       "       [' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n'],\n",
       "       ['i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd'],\n",
       "       ['n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' '],\n",
       "       ['t', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't'],\n",
       "       ['o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a'],\n",
       "       [' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r'],\n",
       "       ['c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g'],\n",
       "       ['o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e'],\n",
       "       ['n', 't', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't'],\n",
       "       ['t', 'e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' '],\n",
       "       ['e', 'x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c'],\n",
       "       ['x', 't', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h'],\n",
       "       ['t', ' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a'],\n",
       "       [' ', 'a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r'],\n",
       "       ['a', 'n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a'],\n",
       "       ['n', 'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c'],\n",
       "       ['d', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't'],\n",
       "       [' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e'],\n",
       "       ['t', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r'],\n",
       "       ['a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' '],\n",
       "       ['r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's'],\n",
       "       ['g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e'],\n",
       "       ['e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q'],\n",
       "       ['t', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u'],\n",
       "       [' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e'],\n",
       "       ['c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n'],\n",
       "       ['h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c'],\n",
       "       ['a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e'],\n",
       "       ['r', 'a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' '],\n",
       "       ['a', 'c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n'],\n",
       "       ['c', 't', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u'],\n",
       "       ['t', 'e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm'],\n",
       "       ['e', 'r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b'],\n",
       "       ['r', ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e'],\n",
       "       [' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r'],\n",
       "       ['s', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotLanguageModel().inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n', 'g', ' ', 'i', 'n', 't', 'o', ' ', 'c', 'o', 'n', 't', 'e', 'x', 't', ' ', 'a', 'n',\n",
       "       'd', ' ', 't', 'a', 'r', 'g', 'e', 't', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r',\n",
       "       ' ', 's', 'e', 'q', 'u', 'e', 'n', 'c', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', '.'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotLanguageModel().inverse_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, check the memory used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481296384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes + Y.nbytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
